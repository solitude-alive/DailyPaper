# The Latest Daily Papers - Date: 2025-01-22
## Highlight Papers
### **[Title: TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space](http://arxiv.org/abs/2501.12224v1)**
- **Summary**: **Summary:** The paper introduces TokenVerse, a novel framework for multi-concept personalization using a pre-trained text-to-image (T2I) diffusion model. TokenVerse can successfully disentangle intricate visual elements from a single image, allowing users to generate new images that combine concepts derived from multiple images. The key innovation is the utilization of a DiT-based T2I model where text input influences the image generation process through attention and modulation techniques. The authors note that their modulation space is semantic, providing localized control over various complex concepts, including objects, accessories, materials, poses, and lighting. The framework functions by optimizing the relationship between image input and text descriptions to map specific words to distinct directions in this modulation space, effectively allowing for the generation of personalized images. The effectiveness of TokenVerse is demonstrated in challenging personalization scenarios, outperforming existing methods. **Critical Evaluation:** The novelty of TokenVerse lies in its approach to handling multiple images that can embody multiple concepts, which is a notable advancement over previous methods. This ability to combine and modulate concepts semantically and effectively through a pre-trained model suggests significant potential for fine-tuned personalization in image generation. Additionally, the identification of distinct directions for different concepts in the modulation space is a progressive step that may inspire future research in T2I tasks and personalized content creation. However, while the technical advances are impressive, the paper may not sufficiently explore the limitations of the method or its applicability in real-world scenarios. For example, while the framework claims to provide localized control, it remains to be seen how it performs in a broader range of contexts or with more complex scenes that may not fit neatly into the defined modulation categories. Furthermore, the practical usability of the method needs clarification, including the computational efficiency and the resources required for leveraging such a framework in everyday applications. In terms of impact, the paper seems to be positioned well within the evolving field of AI-driven image generation, particularly with increasing demand for personalized content across various platforms. However, it would benefit from a deeper discussion of future work or potential challenges that may arise in extending the framework. In summary, the strengths of TokenVerse include its innovative approach to multi-concept personalization and the practical utility demonstrated through its application. However, the paper somewhat under-reports the challenges and future directions necessary for broader implementation. Based on these considerations, I would assign the paper a score of 8. **Score: 8**
- **Score**: 8/10

### **[Title: InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models](http://arxiv.org/abs/2501.12231v1)**
- **Summary**: **Summary:** The paper presents InsTALL, a Context-aware Instructional Task Assistant that utilizes multi-modal large language models to enhance task assistance by incorporating visual data and understanding context. InsTALL is trained using both task videos and corresponding textual data, which enables it to recognize and predict actions within tasks effectively. The model notably extracts task graphs from video data, integrating this information throughout training and inference processes. Results indicate that InsTALL achieves state-of-the-art performance on various sub-tasks such as task and action recognition, next action prediction, and plan prediction. Furthermore, InsTALL demonstrates superior capabilities in automated error identification tasks compared to existing methods. --- **Critical Evaluation:** **Novelty and Significance:** The paper presents a significant advancement in the intersection of multimodal learning and task assistance technologies. By integrating visual modalities with language input to create context-aware assistants, it addresses a current gap in the literature where many existing systems primarily focus on text or audio inputs without fully utilizing visual context. The notion of constructing task graphs from video data is particularly innovative, as it suggests a structured approach to understanding complex tasks - an area that has seen limited exploration in prior research.  **Strengths:** - **Innovative Approach:** The use of multi-modal inputs for context-aware assistance signifies a novel approach that could enhance user interaction and support in various applications, particularly those involving complex, multi-step processes. - **Comprehensive Evaluation:** The paper rigorously evaluates InsTALL across several sub-tasks, demonstrating its capability to outperform existing models. This thorough benchmarking strengthens its claims of superiority. - **Practical Implications:** By improving real-time assistance capabilities, InsTALL could have practical applications in education, training, and remote assistance, potentially leading to better outcomes in user tasks. **Weaknesses:** - **Generalizability Concerns:** While the results reported are promising, the evaluation may be limited in diversity regarding the types of tasks and user scenarios assessed. The robustness of InsTALL in a broader range of real-world contexts remains to be proven. - **Dependency on Visual Data:** The reliance on visual input raises challenges around usability in situations where visual data is not readily available or where capturing video may be intrusive. - **Complexity of Implementation:** Although the paper presents a robust model, the complexity of implementation for both training and inference might limit accessibility for developers who might want to apply this technology in practical applications. **Overall Impact:** InsTALL has the potential to significantly influence the development of virtual assistants and educational tools by providing effective context-aware support that integrates various modalities. Furthermore, the advancements in error identification and action prediction can lead to more intelligent systems capable of supporting individuals in diverse scenarios. **Score: 8**   This score reflects the paper's strong novelty in multi-modal task assistance and rigorous evaluation while noting concerns about usability in broader contexts and potential implementation challenges.
- **Score**: 8/10

### **[Title: FOCUS: First Order Concentrated Updating Scheme](http://arxiv.org/abs/2501.12243v1)**
- **Summary**: ### Summary of the Paper The paper titled "FOCUS: First Order Concentrated Updating Scheme" explores methods to enhance the pre-training of large language models (LLMs) by addressing the limitations found in existing optimizers such as Adam when faced with gradient noise. The authors hypothesize that the loss landscape during pre-training behaves like a narrowing valley, where noise levels can significantly impact optimization performance. Experiments with synthetic loss functions reveal that under conditions of high gradient query noise, Adam's reduction of effective step size contributes to suboptimal performance compared to the Signum optimizer. To address this issue, the authors introduce FOCUS, an optimizer that combines features from Signum with an attraction mechanism towards moving average parameters, promoting larger step sizes while maintaining stability in the presence of noise. Their empirical results, particularly in training GPT-2, show that FOCUS outperforms Signum in stability and is faster than Adam. The findings encourage further investigation into the role of gradient noise in LLM training. ### Evaluation of Novelty and Significance **Novelty:** 1. **Innovative Approach to Noise Handling:** The introduction of FOCUS represents a significant innovation by combining the strengths of existing optimization techniques (Signum and Adam) while also addressing a notable gap regarding gradient noise. 2. **Experimental Insights:** The use of synthetic loss functions to investigate optimizer performance under varying conditions of noise adds a unique dimension to the understanding of how different optimizers behave, which is not commonly addressed in the literature. **Significance:** 1. **Potential Impact on LLM Training:** By postulating that gradient noise is an underappreciated factor in the performance of optimizers, the paper opens avenues for future research that could lead to more efficient training approaches for LLMs. 2. **Practical Applications:** The demonstration of FOCUS’s effectiveness, particularly with a widely-used model like GPT-2, indicates practical implications for trainers and researchers in improving performance and stability in various machine learning applications. **Strengths:** - The alignment of theoretical insights with empirical results enhances the validity of the proposed method. - Clear motivation and justification for exploring new optimization strategies in LLM training, rooted in established concepts. **Weaknesses:** - While the paper discusses the implications of gradient noise, it could provide a more detailed analysis of varying noise levels in real-world scenarios, beyond the synthetic benchmarks. - Additional comparisons with other emerging optimizers and more extensive experiments on different models would strengthen the claims made about performance improvements. Overall, the paper presents a valuable contribution to the field, particularly for those involved in the optimization challenges of LLMs. Its combination of theoretical exploration, empirical validation, and focus on a relevant problem makes it a meaningful addition to current research. **Score: 8**  ### Justification for the Score: The score of 8 reflects a robust contribution but acknowledges areas that could benefit from further elaboration and evidence. The novelty is significant in terms of exploring an often-overlooked aspect (gradient noise), and the results demonstrate clear performance benefits of the proposed optimizer, FOCUS. However, the paper could be strengthened by more comprehensive analysis and wider exploratory comparisons, which somewhat limit its overall impact. Therefore, while it provides a noteworthy step forward, there remains room for additional development and verification within the broader optimization landscape for LLMs.
- **Score**: 8/10

### **[Title: VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models](http://arxiv.org/abs/2501.12267v1)**
- **Summary**: **Summary of the Paper:** The paper introduces VipDiff, a novel framework for video inpainting that employs training-free denoising diffusion models. Addressing the limitations of traditional video inpainting techniques that rely on optical flow for pixel propagation, VipDiff effectively handles large masked areas, which often suffer from artifacts due to the absence of pixel correspondences in their centers. This framework uniquely conditions diffusions on the reverse process, utilizing optical flow to extract valid pixels from reference frames. As a result, it optimizes randomly sampled Gaussian noise into temporally coherent inpainted outputs, allowing for diverse results by sampling different noise patterns. Experimental results indicate that VipDiff surpasses existing state-of-the-art methods in both spatial-temporal coherence and fidelity in video inpainting. **Critical Evaluation:** **Novelty:**  VipDiff presents an innovative approach by integrating diffusion models into video inpainting without requiring extensive training or fine-tuning, which is a notable departure from existing methods that necessitate predefined training data. The idea of conditioning the diffusion process utilizing optical flow for coherent results is also a fresh perspective, highlighting the interoperability of diffusion models with temporal constraints in video data. **Significance:**  The significance of VipDiff lies in its potential to alleviate common pitfalls in video inpainting—namely, the generation of artifacts in regions where large areas need reconstruction. This addresses crucial practical challenges faced in video editing and restoration fields, potentially leading to applications in film post-production, archival video restoration, and real-time streaming enhancements. **Strengths:**  - The approach is innovative and leverages cutting-edge techniques in the realm of generative models without the burdensome requirements of training. - The focus on temporal coherence addresses a substantial gap in existing methods. - The experimental results provided are quantitative, showcasing significant improvements over current technologies. **Weaknesses:** - While the framework is compelling, the lack of a comprehensive training component may limit its application versatility compared to methods that can be fine-tuned for specific visual characteristics in different types of videos. - The paper could benefit from more qualitative assessments or comparisons, such as user studies, to confirm perceptions of fidelity beyond numerical results. - Depending on the experimental setup and random noise sampling, there may be limitations on the diversity of results, which warrants further exploration in various contexts. Based on these considerations, VipDiff is assessed as a notable contribution to the field of video inpainting, particularly in terms of addressing existing weaknesses in coherence and fidelity. However, the reliance on a purely training-free methodology may present long-term performance concerns in specialized applications. **Score: 8**
- **Score**: 8/10

### **[Title: Test-time regression: a unifying framework for designing sequence models with associative memory](http://arxiv.org/abs/2501.12352v1)**
- **Summary**: **Summary:** The paper presents a unifying framework for understanding various architectures used in sequence modeling through the lens of associative memory and regression at test-time. The authors argue that effective sequence models must have the capability for associative recall, which they show is linked to the ability to memorize input tokens. They analyze numerous contemporary architectures, such as linear attention models and state-space models, framing them as different strategies for performing test-time regression. The paper outlines three design choices that dictate an architecture's performance: the weight of associations, the nature of the regressor function, and the optimization method used. This approach not only provides insights into model design but also offers theoretical validation for existing methods, paving the way for advanced developments in sequence modeling. **Critical Evaluation:** The paper's central thesis provides a significant contribution to the field by proposing a coherent framework that connects a variety of seemingly disparate sequence modeling techniques. The emphasis on associative memory as a key component in sequence modeling is innovative and highlights an often-overlooked aspect of model performance—recall of learned inputs. One of the strengths of the paper is its ability to derive insights from existing models and establish connections that may inspire further research. The treatment of models like linear attention and softmax attention is particularly notable, as it contextualizes these methods within a broader theoretical framework. Additionally, the authors' introduction of regression as a critical function at test-time could stimulate new research directions aimed at more effective design principles. However, while the framework is unifying, it risks oversimplifying the complexities inherent in the design and behavior of advanced sequence models. Moreover, the empirical validation of the framework and its propositions could be stronger; the paper largely relies on theoretical underpinnings without detailed experiments to substantiate the claims regarding model performance or efficiency. The theoretical connections drawn in the paper, such as the justification for QKNorm, are valuable but could be built upon with more rigorous analytical or empirical studies. As a result, while the framework is promising, the actual application of it in new model development and real-world scenarios remains to be fully tested. In summary, while the paper articulates a compelling vision for understanding and integrating sequence models, the potential impact may be somewhat tempered by the need for more empirical grounding.  **Score: 8**
- **Score**: 8/10

### **[Title: Parallel Sequence Modeling via Generalized Spatial Propagation Network](http://arxiv.org/abs/2501.12381v1)**
- **Summary**: **Summary:** The paper introduces the Generalized Spatial Propagation Network (GSPN), an innovative attention mechanism designed to address limitations faced by existing models in efficiently processing multi-dimensional, spatially coherent image data. Unlike conventional methods that transform multi-dimensional data into 1D sequences, GSPN retains 2D spatial structures and deploys a line-scan approach to establish dense pairwise connections. This mechanism implements the Stability-Context Condition to maintain stable, context-aware data propagation, effectively reducing the sequence length to $\sqrt{N}$ for square maps, thereby improving computational efficiency. The GSPN operates with learnable, input-dependent weights and eliminates the need for positional embeddings, resulting in enhanced spatial fidelity. Its performance outstrips current standards in several vision tasks, exemplified by accelerated generation in SD-XL models by more than 84 times for 16K image outputs. **Critical Evaluation:** The introduction of GSPN marks a notable advancement in the field of attention mechanisms, particularly for vision tasks where spatial coherence is crucial. The emphasis on maintaining 2D spatial structures while reducing computational demands presents a compelling challenge to traditional transformer models that typically flatten data for processing. The Stability-Context Condition represents a novel conceptual framework that aims to optimize propagation across 2D sequences, which could inspire further research into context-aware models for various applications. **Strengths:** 1. **Novel Approach:** GSPN introduces a fundamentally new way to approach attention mechanisms that can directly benefit tasks uniquely tied to spatial representational fidelity. 2. **Computational Efficiency:** The significant reduction in effective sequence length and improved speed for high-resolution image generation highlights GSPN's practical advantages, potentially enabling faster workflows in real-world applications. 3. **Performance Metrics:** Achieving state-of-the-art results across diverse vision tasks lends credibility to the methodologies adopted and underscores the competitive edge of GSPN over prior models. **Weaknesses:** 1. **Complexity and Scalability:** While GSPN shows promise, how it scales with even larger datasets or more intricate tasks remains an open question. The multi-fold increase in computational performance should be weighed against potential complexities arising from its dense connection strategy. 2. **Dependence on Specific Context:** The reliance on the Stability-Context Condition may pose challenges in varied applications with highly dynamic spatial relationships; additional empirical evidence across a broader spectrum of tasks would strengthen its validity. 3. **Comparison with Existing Models:** While claimed improvements in specific tasks are impressive, a more exhaustive comparison against contemporary models in diverse settings and datasets would provide a better insight into its overall effectiveness. This paper represents a substantial contribution to the field of deep learning and computer vision. Its novel approach could inspire future research, although the practical implications of broader applications still need to be evaluated. Given the strengths and room for further validation, I assign a score of 8. **Score: 8**
- **Score**: 8/10

### **[Title: DiffDoctor: Diagnosing Image Diffusion Models Before Treating](http://arxiv.org/abs/2501.12382v1)**
- **Summary**: **Summary of the Paper:** The paper introduces **DiffDoctor**, a novel two-stage pipeline designed to enhance image diffusion models by reducing the production of artifacts. The first stage involves creating a robust artifact detection system, supported by a dataset of over 1 million flawed synthesized images and an efficient human-in-the-loop annotation strategy that ensures a balanced representation of defects. The second stage integrates the developed artifact detector to generate per-pixel confidence maps for the image generation process, allowing for more focused refinement of the diffusion model. The authors demonstrate through extensive experiments that their approach effectively reduces artifacts in text-to-image diffusion models, supporting the proposed diagnose-then-treat paradigm. **Critical Evaluation:** **Novelty:**  The novelty of DiffDoctor lies in its dual approach—first diagnosing the specific locations of artifacts and then treating them rather than relying solely on holistic quality assessments. This targeted methodology is a marked advancement over existing strategies that do not account for spatial variations in defects. Additionally, the creation of a large dataset specifically for artifact detection contributes to the field by providing necessary resources for development and evaluation. **Significance:** In the context of the rapidly evolving field of image synthesis, as seen with the growing interest in diffusion models, producing cleaner images is paramount. The proposed methodology addresses a significant issue—artifacts—that hinder the full potential of these technologies in practical applications. By introducing a systematic process for detecting and correcting defects, DiffDoctor could enhance the reliability of image generation tools, which may lead to wider adoption in various fields such as gaming, film, and virtual reality. **Strengths:** - The large-scale dataset and human-in-the-loop annotation process are well-conceived and likely to yield high-quality training for the artifact detection model. - The rigorous experimental setup provides compelling evidence for the proposed method's effectiveness, enhancing confidence in the results. **Weaknesses:** - The study focuses exclusively on text-to-image diffusion models, which may limit the general applicability of the findings to other diffusion tasks or models. - The potential computational overhead introduced by the two-stage process may raise concerns about efficiency and feasibility in real-time applications. **Potential Influence:** Given the growing importance of mitigating artifacts in image synthesis, DiffDoctor could set a precedent for future research focused on defect identification and correction in generative models. It highlights the importance of not only generating high-quality images but also understanding and managing the failures of these models. **Score: 8** This score reflects a balanced view of the paper's contributions and limitations. While indeed innovative and addressing a relevant problem within the field of image diffusion models, there remains a gap in applicability across various contexts and model types that future research will need to address. The strong methodological approach and the potential impact on the domain bolster its overall significance, yet further generalization and efficiency improvements would enhance its utility.
- **Score**: 8/10

### **[Title: InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling](http://arxiv.org/abs/2501.12386v1)**
- **Summary**: **Summary:** The paper titled "InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling" presents an advancement in video multimodal large language models (MLLMs) with the introduction of long and rich context (LRC) modeling. The authors develop a new iteration of InternVideo, which enhances the model's ability to interpret fine-grained details and understand long-term temporal structures in videos. This is achieved by integrating dense task-specific annotations through direct preference optimization, and creating compact spatiotemporal representations via adaptive hierarchical token compression. The experimental results indicate that this approach significantly improves the model's performance across various video understanding benchmarks, extending its capacity to process inputs at least six times longer than previous versions, while also enhancing capabilities like object tracking and segmentation. The study emphasizes the critical role of multimodal context richness in enhancing the effectiveness of MLLMs, providing valuable insights for subsequent research. **Critical Evaluation:** The paper presents several strengths: 1. **Technical Innovation**: The integration of long and rich context modeling addresses a notable limitation in existing video MLLMs, where processing long video sequences and appreciating fine details often pose significant challenges. The novel use of dense annotations and adaptive token compression represents a useful contribution to the field. 2. **Empirical Validation**: The demonstration of improved performance benchmarks lends credibility to the proposed methods. The results showing a sixfold increase in input memory are particularly significant, indicating a substantial advancement in the model’s capabilities. 3. **Potential for Further Research**: By emphasizing multimodal context richness, the paper opens pathways for future explorations in video understanding and MLLM architectures. However, there are some weaknesses to consider: 1. **Comparative Analysis**: While the results are compelling, a more rigorous comparative analysis with other leading MLLM frameworks could strengthen the paper by positioning the contributions more clearly against existing state-of-the-art models. 2. **Generalizability**: The focus on specific benchmarks may limit the perceived robustness of the findings. It would benefit the authors to validate their model across a broader set of datasets and tasks to ensure versatility in diverse real-world applications. 3. **Complexity of Implementation**: The methods proposed, given their innovative nature, may introduce computational complexity that could hinder practical application. A discussion on computational trade-offs and efficiency could further substantiate the impact of their work. Overall, while the paper contributes important insights and methodologies to the field of video MLLMs, the relative novelty and significance could be assessed further through comparative frameworks and broader validation.  **Score: 8**
- **Score**: 8/10

### **[Title: Towards Affordance-Aware Articulation Synthesis for Rigged Objects](http://arxiv.org/abs/2501.12393v1)**
- **Summary**: ### Summary of the Paper The paper titled "Towards Affordance-Aware Articulation Synthesis for Rigged Objects" addresses the challenge of articulating rigged objects in a way that is both realistic and context-sensitive. These objects, prevalent in the artistic and animation pipelines, can be difficult to pose naturally without extensive input from skilled artists. The authors introduce a novel system called A3Syn, which automates the synthesis of articulation parameters for various rigged objects based on specific contexts defined by environment meshes and text prompts. A3Syn employs a 2D inpainting diffusion model and advanced control techniques to generate affordance-aware postures. It also innovates a robust bone correspondence alignment approach using differentiable rendering and semantic matching. The process is designed to operate efficiently, delivering results in minutes without the need for extensive training data or rigid topological constraints on the rigs used. ### Critical Evaluation of Novelty and Significance The paper makes several notable contributions to the field of computer graphics and animation. The approach of synthesizing articulation based on environmental context and affordance awareness is quite innovative, addressing a significant limitation in current rigged object manipulation systems. The use of a 2D inpainting diffusion model for generating complex poses is a fresh perspective that suggests potential for broader applications beyond the specific case of rigged objects. **Strengths:** 1. **Novelty of Approach**: The integration of inpainting diffusion models and the lack of strict topological assumptions represent a significant advancement. This opens doors for more flexible and varied applications in animations and simulations. 2. **Efficiency**: The ability of A3Syn to produce results within minutes while maintaining stability and plausibility in output is a strong advantage, particularly in high-demand creative environments. 3. **Broad Applicability**: The system’s compatibility with a wide range of rigged objects found online enhances its practical relevance and usability. **Weaknesses:** 1. **Training Data Limitations**: The claim of operating with limited training data, while ambitious and beneficial, may lead to challenges in the robustness of the models, especially in edge cases where unique rig variations are presented. 2. **Evaluation Metrics**: The paper could fall short regarding the quantitative evaluation of the synthesized articulations; stronger metrics could reinforce claims about convergence and plausibility. 3. **Lack of Comparative Analysis**: There is minimal discussion on how A3Syn compares to existing methods in terms of both qualitative output and computational efficiency, which could leave some questions around its relative performance. **Potential Influence**: This work has the potential to significantly impact fields such as game design, animation, and virtual reality, where the need for dynamic and realistic representations of objects is increasing. If the methodologies presented in A3Syn are adopted and further developed, they could change the landscape of rigged object utilization in these areas. ### Conclusion Overall, while the paper presents a compelling foundation and a clear advancement in affordance-aware articulation for rigged objects, it has room for improvement in terms of validation and comparative analysis. Its innovative aspect, particularly with the synthesis methods and operational efficiency, however, positions it as a noteworthy contribution to the field of computer graphics. **Score: 8**
- **Score**: 8/10

## Other Papers
### **[Title: Leveraging Large Language Models for Realizing Truly Intelligent User Interfaces](http://arxiv.org/abs/2501.12221v1)**
### **[Title: TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space](http://arxiv.org/abs/2501.12224v1)**
### **[Title: CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning](http://arxiv.org/abs/2501.12226v1)**
### **[Title: InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models](http://arxiv.org/abs/2501.12231v1)**
### **[Title: FOCUS: First Order Concentrated Updating Scheme](http://arxiv.org/abs/2501.12243v1)**
### **[Title: VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models](http://arxiv.org/abs/2501.12267v1)**
### **[Title: Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement](http://arxiv.org/abs/2501.12273v1)**
### **[Title: MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in Dynamic Sensing Networks](http://arxiv.org/abs/2501.12281v1)**
### **[Title: LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations](http://arxiv.org/abs/2501.12300v1)**
### **[Title: Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration](http://arxiv.org/abs/2501.12332v1)**
### **[Title: Test-time regression: a unifying framework for designing sequence models with associative memory](http://arxiv.org/abs/2501.12352v1)**
### **[Title: Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL](http://arxiv.org/abs/2501.12372v1)**
### **[Title: Parallel Sequence Modeling via Generalized Spatial Propagation Network](http://arxiv.org/abs/2501.12381v1)**
### **[Title: DiffDoctor: Diagnosing Image Diffusion Models Before Treating](http://arxiv.org/abs/2501.12382v1)**
### **[Title: Audio Texture Manipulation by Exemplar-Based Analogy](http://arxiv.org/abs/2501.12385v1)**
### **[Title: InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling](http://arxiv.org/abs/2501.12386v1)**
### **[Title: GPS as a Control Signal for Image Generation](http://arxiv.org/abs/2501.12390v1)**
### **[Title: Towards Affordance-Aware Articulation Synthesis for Rigged Objects](http://arxiv.org/abs/2501.12393v1)**
